{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMxjIZwZRrSQKf5kxMuuIJJ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSjwMvEaNp8a"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "58AorYAvNrnv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('enbn.json', 'r') as file:\n",
        "    dataenbn = json.load(file)\n",
        "\n",
        "with open('bnen.json', 'r') as file:\n",
        "    databnen = json.load(file)\n",
        "\n",
        "with open('enhi.json', 'r') as file:\n",
        "    dataenhi = json.load(file)\n",
        "\n",
        "with open('hien.json', 'r') as file:\n",
        "    datahien = json.load(file)"
      ],
      "metadata": {
        "id": "ct0ubH3HNrkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "y9aJtAQJNriE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import MT5ForConditionalGeneration, MT5Tokenizer, DataCollatorForSeq2Seq"
      ],
      "metadata": {
        "id": "o8b4mP9SNrfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = MT5Tokenizer.from_pretrained(\"google/mt5-small\")\n",
        "model = MT5ForConditionalGeneration.from_pretrained(\"google/mt5-small\")"
      ],
      "metadata": {
        "id": "_NegmyLZNrdF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source_lang = \"en\"\n",
        "target_lang = \"bn\"\n",
        "prefix = \"translate English to Bengali: \"\n",
        "\n",
        "inputs = [prefix + example[source_lang] for example in dataenbn]\n",
        "targets = [example[target_lang] for example in dataenbn]\n",
        "model_inputs_en1 = tokenizer(inputs, max_length=128, truncation=True)\n",
        "\n",
        "with tokenizer.as_target_tokenizer():\n",
        "  labels_bn = tokenizer(targets, max_length=128, truncation=True)"
      ],
      "metadata": {
        "id": "ltr2jBkgNrah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source_lang = \"en\"\n",
        "target_lang = \"hi\"\n",
        "prefix = \"translate English to Hindi: \"\n",
        "\n",
        "inputs = [prefix + example[source_lang] for example in dataenhi]\n",
        "targets = [example[target_lang] for example in dataenhi]\n",
        "model_inputs_en2 = tokenizer(inputs, max_length=128, truncation=True)\n",
        "\n",
        "with tokenizer.as_target_tokenizer():\n",
        "  labels_hi = tokenizer(targets, max_length=128, truncation=True)"
      ],
      "metadata": {
        "id": "RzH8CBPRNrYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source_lang = \"hi\"\n",
        "target_lang = \"en\"\n",
        "prefix = \"translate Hindi to English: \"\n",
        "\n",
        "inputs = [prefix + example[source_lang] for example in datahien]\n",
        "targets = [example[target_lang] for example in datahien]\n",
        "\n",
        "with tokenizer.as_target_tokenizer():\n",
        "  model_inputs_hi = tokenizer(inputs, max_length=128, truncation=True)\n",
        "\n",
        "labels_en1 = tokenizer(targets, max_length=128, truncation=True)"
      ],
      "metadata": {
        "id": "58ZH3NFxNrVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source_lang = \"bn\"\n",
        "target_lang = \"en\"\n",
        "prefix = \"translate Bengali to English: \"\n",
        "\n",
        "inputs = [prefix + example[source_lang] for example in databnen]\n",
        "targets = [example[target_lang] for example in databnen]\n",
        "\n",
        "with tokenizer.as_target_tokenizer():\n",
        "  model_inputs_bn = tokenizer(inputs, max_length=128, truncation=True)\n",
        "\n",
        "labels_en2 = tokenizer(targets, max_length=128, truncation=True)"
      ],
      "metadata": {
        "id": "8fXXVzB9NrTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenate model inputs\n",
        "model_inputs = {\n",
        "    \"input_ids\": model_inputs_en1[\"input_ids\"] + model_inputs_en2[\"input_ids\"] + model_inputs_hi[\"input_ids\"] + model_inputs_bn[\"input_ids\"],\n",
        "    \"attention_mask\": model_inputs_en1[\"attention_mask\"] + model_inputs_en2[\"attention_mask\"] + model_inputs_hi[\"attention_mask\"] + model_inputs_bn[\"attention_mask\"]\n",
        "}\n",
        "\n",
        "# Concatenate labels\n",
        "labels = labels_en1[\"input_ids\"] + labels_hi[\"input_ids\"] + labels_bn[\"input_ids\"] + labels_en2[\"input_ids\"]\n",
        "\n",
        "# Add labels to model_inputs\n",
        "model_inputs[\"labels\"] = labels"
      ],
      "metadata": {
        "id": "BP-q9GlwNrQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "# Create a dictionary containing your tokenized inputs and labels\n",
        "data = {\n",
        "    \"input_ids\": model_inputs[\"input_ids\"],\n",
        "    \"attention_mask\": model_inputs[\"attention_mask\"],\n",
        "    \"labels\": model_inputs[\"labels\"]\n",
        "}\n",
        "\n",
        "# Convert the dictionary to a dataset object\n",
        "dataset = Dataset.from_dict(data)"
      ],
      "metadata": {
        "id": "l6WW3TP4NrOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "ZqBzyoGxNrMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.train_test_split(test_size=0.2)"
      ],
      "metadata": {
        "id": "J3vnK9t0NrHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset)"
      ],
      "metadata": {
        "id": "x0IcZu8dNrFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
      ],
      "metadata": {
        "id": "jEI8fXn8OOG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers[torch]"
      ],
      "metadata": {
        "id": "O78-xZMrOODb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate -U"
      ],
      "metadata": {
        "id": "hVLPNSuxOOBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show transformers\n",
        "!pip show accelerate"
      ],
      "metadata": {
        "id": "zXidl6EcON_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall transformers accelerate\n",
        "!pip install transformers[torch]"
      ],
      "metadata": {
        "id": "VHsKuCX5PYWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import MT5ForConditionalGeneration, MT5Tokenizer\n",
        "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
        "\n",
        "# Define and initialize your T5 model and tokenizer\n",
        "tokenizer = MT5Tokenizer.from_pretrained(\"google/mt5-small\")\n",
        "model = MT5ForConditionalGeneration.from_pretrained(\"google/mt5-small\")"
      ],
      "metadata": {
        "id": "GBNtynd8ON9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForSeq2Seq\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
      ],
      "metadata": {
        "id": "zrFx7fq7ON6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "print(transformers.__version__)"
      ],
      "metadata": {
        "id": "US0B0kz5ON4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
        "\n",
        "# Define your training arguments\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=3,\n",
        "    #save_steps=1000,\n",
        "    num_train_epochs=1,\n",
        "    fp16=False,\n",
        ")\n",
        "\n",
        "# Instantiate the Seq2SeqTrainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset[\"train\"],  # Use the created dataset\n",
        "    eval_dataset=dataset[\"test\"],   # You can use the same dataset for evaluation as well\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# Start the training\n",
        "trainer.train()\n",
        "\n",
        "# Save the final model checkpoint\n",
        "trainer.save_model(\"./final_model\")\n"
      ],
      "metadata": {
        "id": "9K8XcSRGON2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('hibn.json', 'r') as file:\n",
        "    datahibn = json.load(file)"
      ],
      "metadata": {
        "id": "oTTEthDiONz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source_lang = \"hi\"\n",
        "target_lang = \"bn\"\n",
        "prefix = \"translate Hindi to Bengali: \"\n",
        "\n",
        "inputs = [prefix + example[source_lang] for example in datahibn]\n",
        "targets = [example[target_lang] for example in datahibn]\n",
        "\n",
        "with tokenizer.as_target_tokenizer():\n",
        "  model_inputs_hi = tokenizer(inputs, max_length=128, truncation=True)\n",
        "with tokenizer.as_target_tokenizer():\n",
        "  labels_bn = tokenizer(targets, max_length=128, truncation=True)\n"
      ],
      "metadata": {
        "id": "Ag-XPftSOe2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_inputs_hi = {\n",
        "    \"input_ids\": model_inputs_hi[\"input_ids\"],\n",
        "    \"attention_mask\": model_inputs_hi[\"attention_mask\"]\n",
        "}"
      ],
      "metadata": {
        "id": "BfdtNJRgOezJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs_hi = Dataset.from_dict(model_inputs_hi)"
      ],
      "metadata": {
        "id": "qj7Vsm3qOexF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(inputs_hi['input_ids'][0])\n",
        "print(inputs_hi['input_ids'][1])\n",
        "print(inputs_hi['input_ids'][2])\n",
        "print(inputs_hi['input_ids'][3])"
      ],
      "metadata": {
        "id": "CQpGBh-zOwTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fine_tuned_model = MT5ForConditionalGeneration.from_pretrained(\"./final_model\")"
      ],
      "metadata": {
        "id": "5RNXK9CXOeu-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "v549Ky-ZO3wK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the preprocessed Hindi sentences\n",
        "hindi_inputs = inputs_hi['input_ids']\n",
        "# Determine the maximum sequence length\n",
        "max_length = max(len(seq) for seq in hindi_inputs)\n",
        "\n",
        "\n",
        "# Pad sequences to the maximum length\n",
        "padded_inputs = [seq + [tokenizer.pad_token_id] * (max_length - len(seq)) for seq in hindi_inputs]\n",
        "\n",
        "# Convert to tensor\n",
        "hindi_inputs_tensor = torch.tensor(padded_inputs)\n",
        "hindi_inputs_tensor = torch.tensor(hindi_inputs_tensor)\n",
        "\n",
        "# Perform inference\n",
        "with torch.no_grad():\n",
        "    outputs = fine_tuned_model.generate(input_ids=hindi_inputs_tensor,\n",
        "                             max_length=128,  # Adjust max_length as needed\n",
        "                             num_beams=4,     # Adjust num_beams as needed\n",
        "                             early_stopping=True,\n",
        "                             num_return_sequences=1)\n",
        "\n",
        "# Decode the model outputs to obtain translated Bengali sentences\n",
        "with tokenizer.as_target_tokenizer():\n",
        "  translated_bengali_sentences = tokenizer.batch_decode(outputs, skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "oWSFjnNMO3i4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(translated_bengali_sentences)"
      ],
      "metadata": {
        "id": "KZg65vTgO3L3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(outputs)"
      ],
      "metadata": {
        "id": "-r4u2zculd2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translated_bn_json = json.dumps(translated_bengali_sentences, ensure_ascii = False)"
      ],
      "metadata": {
        "id": "AsoZmhY4O9zp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path to save the JSON file\n",
        "output_json_path = 'translated_bn_json.json'\n",
        "\n",
        "# Save JSON data to a file\n",
        "with open(output_json_path, 'w', encoding='utf8') as json_file:\n",
        "    json_file.write(translated_bn_json)\n",
        "\n",
        "print(\"Conversion completed. JSON file saved as:\", output_json_path)"
      ],
      "metadata": {
        "id": "P68PbZTRO9nq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "references = [sample['bn'] for sample in datahibn]"
      ],
      "metadata": {
        "id": "9aWQq5jDsKtc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(references)"
      ],
      "metadata": {
        "id": "fvYV1eZms9Jc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate corpus BLEU score\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "bleu_score = corpus_bleu(references, translated_bengali_sentences)\n",
        "print(\"BLEU Score:\", bleu_score)"
      ],
      "metadata": {
        "id": "OBv736mfsLyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_metric\n",
        "\n",
        "bleu = load_metric(\"bleu\")\n",
        "\n",
        "sum_bleu = 0\n",
        "for sent in range(0, len(translated_bengali_sentences)):\n",
        "  pred = [[i for i in translated_bengali_sentences[sent]]]\n",
        "  ref = [[[i for i in references[sent]]]]\n",
        "  score = bleu.compute(predictions = pred, references = ref)\n",
        "  sum_bleu = sum_bleu + score['bleu']\n",
        "  print(score)\n",
        "\n",
        "print(sum_bleu)\n",
        "print(sum_bleu/100)"
      ],
      "metadata": {
        "id": "Ge0Mi8IwsU1T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}